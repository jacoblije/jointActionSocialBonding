% File name: trainingExperimentFinalAnalysis.R
% Author: Jacob Taylor
% Date: 20/09/2017
% Description: A script for analysing complete data from trainingExperiment in Weihai and Beijing during August 2016.
% This analysis incorporates performance data coded from video recordings of each experimental session.

\documentclass[english]{article}
\usepackage[margin=1.8cm,a4paper]{geometry}
\usepackage[english]{babel} % hyphenation
\usepackage{amsmath,amssymb,amsthm} % for mathematical
\usepackage{graphicx} % to include graphics




% Main data manipulation tasks: 1) data reduction (group and team measures)
%                               2) group differences
%                               3) manipulation checks

% Main objectives: 1) test study predictions:

%                     1.a relationship between perceived joint action success and team click
%                             1.a.i relationship between session performance variables and perceptions of joint action success / team click?
%                     1.b relationship between expectation violation and team click
%                             1.b.i relationship between session performance variables and expectation violation / team click?
%                     1.c interaction between perceptions of jointActionSuccess and teamClick
%
%                     2.a relationship between team click and social bonding
%                             (2.a.i relationship between performance and social bonding?)
%
%                     3.a relationship between jointActionSuccess and socialBonding?
%                         3.a.i relationship between performance and socialBonding
%                     3.b relationship between expectation violation and socialBonding
%
%                     4.a mediation analysis: teamClick mediates relationship between jointActionSuccess and socialBonding
%                         4.a.i jointActionSuccess mediates relationship between performance and teamClick?
%                     4.b mediation analysis: teamClick mediates relationship between teamPerformanceExpectations and socialBonding
%
%                     2.1.a - 2.4.b ---> re run this looking at pre-post changes in group measures and team measures...
%                     Additional analyses:
%                               1. teamClick mediates relationship between jointAction and generalised socialBonding to team / family?
%                               2.

% setting global settings and loading useful libraries


\begin{document}

<<standardSettings, echo=FALSE, results='hide'>>=
library(knitr)
opts_chunk$set(echo=FALSE,fig.width=3.5, fig.height=3.5,fig.align='center')
library(xtable) # to make Latex tables
install.packages("tableone")
library(tableone)
@

<<dataInput, eval=TRUE, echo=FALSE, results='hide'>>=
setwd("/Users/jacob1/Documents/2017/Research/DPhil/Dissertation/finalDocuments/jointActionSocialBonding/images")
# read in complete data from experiment:
tlc <- read.csv("trainingLongComplete.csv", na.strings = c("", "NA", "n/a"))
twc <- read.csv("trainingWideComplete.csv", na.strings = c("", "NA", "n/a"))
@




\title{Field Experiment}
\date{}
\author{Jacob Taylor & Emma Cohen \\ Institute of Cognitive \and Evolutionary Anthropology, University of Oxford}



\section{Abstract}
This study was designed to further test the hypothesised relationship between joint action, team click, and social bonding.  58 Professional Chinese rugby players (men = 30) participated in a between-subjects design involving two manipulations of a common rugby training drill known as ``invasion drill'' \citep{Passos2011}.  In the ``low uncertainty'' condition, athletes were primed with information to suggest that the training drill would be very easy to complete (2/10 difficulty rating).  In the ``high uncertainty'' condition, athletes were primed to expect the training drill to be relatively difficult (8/10 difficulty).
Pre and post survey measures were recorded, along with video footage of each experimental trial.  It was predicted that those athletes in the high-uncertainty condition would experience higher levels of team click and social bonding owing to higher than expected positive violation of expectations around group performance.
Athletes in the ``low uncertainty'' condition would on average experience less positive violations of expectations, and thus would feel less strongly the phenomenon of ``team click'' and flow-on feelings of social bonding.  Video footage was analysed for evidence of dynamic coupling between co-actors as well as defenders \citep{Schmidt2011,Richardson2012,Passos2012}, and these data were compared to psychological measures.  Results are yet to be fully analysed.

\section{Introduction}
The Opening:
The opening introduces the research question and explains why it is interesting.  The opening, which is usually a paragraph or two in length, introduces the research question and explains why it is interesting. To capture the reader’s attention, researcher Daryl Bem recommends starting with general observations about the topic under study, expressed in ordinary language (not technical jargon)—observations that are about people and their behavior (not about researchers or their research; Bem, 2003).Bem, D. J. (2003).

``The individual who holds two beliefs that are inconsistent with one another may feel uncomfortable. For example, the person who knows that he or she enjoys smoking but believes it to be unhealthy may experience discomfort arising from the inconsistency or disharmony between these two thoughts or cognitions. This feeling of discomfort was called cognitive dissonance by social psychologist Leon Festinger (1957), who suggested that individuals will be motivated to remove this dissonance in whatever way they can (p. 191).''  After capturing the reader’s attention, the opening should go on to introduce the research question and explain why it is interesting. Will the answer fill a gap in the literature? Will it provide a test of an important theory? Does it have practical implications? Giving readers a clear sense of what the research is about and why they should care about it will motivate them to continue reading the literature review—and will help them make sense of it.

The Lit Review:
Immediately after the opening comes the literature review, which describes relevant previous research on the topic and can be anywhere from several paragraphs to several pages in length. However, the literature review is not simply a list of past studies. Instead, it constitutes a kind of argument for why the research question is worth addressing. By the end of the literature review, readers should be convinced that the research question makes sense and that the present study is a logical next step in the ongoing research process.  begin by describing a phenomenon in a general way along with several studies that demonstrate it, then describing two or more competing theories of the phenomenon, and finally presenting a hypothesis to test one or more of the theories. it is extremely important to start with an outline of the main points that you want to make, organized in the order that you want to make them. Or if you are proposing a new theory, then of course you should discuss findings that are consistent with that theory. However, if there are other findings that are inconsistent with it, again, you should discuss them too.


The Closing:
The closing of the introduction—typically the final paragraph or two—usually includes two important elements. The first is a clear statement of the main research question or hypothesis. This statement tends to be more formal and precise than in the opening and is often expressed in terms of operational definitions of the key variables. The second is a brief overview of the method and some comment on its appropriateness.

These considerations lead to the hypothesis that the more bystanders to an emergency, the less likely, or the more slowly, any one bystander will intervene to provide aid. To test this proposition it would be necessary to create a situation in which a realistic “emergency” could plausibly occur. Each subject should also be blocked from communicating with others to prevent his getting information about their behavior during the emergency. Finally, the experimental situation should allow for the assessment of the speed and frequency of the subjects’ reaction to the emergency. The experiment reported below attempted to fulfill these conditions (p. 378).


\section{Method}


\subsection{Participants}
number of women and men, some indication of their age, other demographics that may be relevant to the study, and how they were recruited, including any incentives given for participation.

\subsection{Materials}
This might mean multiple questionnaires, written vignettes that participants read and respond to, perceptual stimuli, and so on. The heading of this subsection can be modified to reflect its content. Instead of “Materials,” it can be “Questionnaires,” “Stimuli,” and so on.

Video recording protocol

\subsection{Design}
The design of a study is its overall structure. What were the independent and dependent variables? Was the independent variable manipulated, and if so, was it manipulated between or within subjects? How were the variables operationally defined?

\subsection{Proceedure}
The procedure is how the study was carried out. It often works well to describe the procedure in terms of what the participants did rather than what the researchers did. For example, the participants gave their informed consent, read a set of instructions, completed a block of four practice trials, completed a block of 20 test trials, completed two questionnaires, and were debriefed and excused.




\section{Results}

1. Preliminary Issues:
Typically it begins with certain preliminary issues. One is whether any participants or responses were excluded from the analyses and why. The rationale for excluding data should be described clearly so that other researchers can decide whether it is appropriate.

Describe the data here?

\subsection{Data Reduction}
Exploratory factor analysis was used to reduce multicolinearity between variables while retaining as much variance as possible in the observed data \citep{Yong2013} (EFA, appendix ~\ref{appendix:EFA}).  Data reduction was performed of the main variables of interest relating to training group performance: jointActionSuccess, individualComponentPerformance, teamClick, socialBonding, technicalCompetence (objective and subjective). In addition, team click and social bonding measures directed at the team as a whole (and not just the specific training group) were reduced, in order to assess pre- to post-experiment variation in generalised bonding to the team.
Procedure for data reduction followed method outlined in ~\ref{subsection:dataReduction}.

\subsubsection{Variables relevant to the specific training group}



\myparagraph{Perceptions of Training Group Joint Action Success}

<<dataReductionGroupPerformance, eval=TRUE, echo=FALSE>>=
#groupPerformanceVariablesSubset:
tlcGroupPerformance <- subset(tlc, select = c(groupDefensiveLine, groupAttackingLine,
                                                  groupSupportPlay, groupOnfieldCommunication))
tlcGroupPerformance.pca <- princomp(na.omit(tlcGroupPerformance))
summary(tlcGroupPerformance.pca)
plot(tlcGroupPerformance.pca)

tlcGroupPerformance.fa <- factanal(~ groupDefensiveLine + groupAttackingLine +
                                            groupSupportPlay + groupOnfieldCommunication, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.groupPerformanceFA <- round(sum(tlcGroupPerformance.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.GroupPerformanceFA <-  round((colSums(tlcGroupPerformance.fa$loadings*tlcGroupPerformance.fa$loadings)/dim(tlcGroupPerformance.fa$loadings)[1]*100), digits = 1)


head(tlcGroupPerformance.fa$scores)
tlc <- cbind(tlc, tlcGroupPerformance.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "jointActionSuccess"
groupPerformanceMatrix <- cor(tlcGroupPerformance, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOgroupPerformanceMatrix <- KMO(groupPerformanceMatrix)
# KMOGroupPerformanceMatrix$MSA is the colum in which the overal KMO is stored
bartlettGroupPerformance <- cortest.bartlett(groupPerformanceMatrix, n = 116) # what is the sample size here?
# ls(BartlettGroupPerformance)
# BartlettGroupPerformance$chisq, BartlettGroupPerformance$df, BartlettGroupPerformance$p.value
# sample size? length(unique(tlc$englishName)*2)
groupPerformanceSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityGroupPerformance <- psych::alpha(groupPerformanceMatrix)
cronAlphaGroupPerformance <- reliabilityGroupPerformance$total[,2]
guttLambdaGroupPerformance <- reliabilityGroupPerformance$total[,3]


#-----------------------Correlation Table----------------------#
library(xtable)
corstarsl <- function(x){
  require(Hmisc)
  x <- as.matrix(x)
  R <- rcorr(x)$r
  p <- rcorr(x)$P

  ## define notions for significance levels; spacing is important.
  mystars <- ifelse(p < .001, "***", ifelse(p < .01, "** ", ifelse(p < .05, "* ", " ")))
  ## trunctuate the matrix that holds the correlations to two decimal
  R <- format(round(cbind(rep(-1.11, ncol(x)), R), 2))[,-1]
  ## build a new matrix that includes the correlations with their apropriate stars
  Rnew <- matrix(paste(R, mystars, sep=""), ncol=ncol(x))
  diag(Rnew) <- paste(diag(R), " ", sep="")
  rownames(Rnew) <- colnames(x)
  colnames(Rnew) <- paste(colnames(x), "", sep="")
  ## remove upper triangle
  Rnew <- as.matrix(Rnew)
  Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
  Rnew <- as.data.frame(Rnew)
  ## remove last column and return the matrix (which is now a data frame)
  Rnew <- cbind(Rnew[1:length(Rnew)-1])
  return(Rnew)
}
@

Items concerning components of group performance in the invasion drill (defence, attack, support play, and on field communication) were subjected to EFA.  Correlations between group component performance items was very high (all r's > \Sexpr{round(min(groupPerformanceMatrix), digits = 2)}), which suggested that one factor was appropriate (see Table ~\ref{tab:jointActionSuccessCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOgroupPerformanceMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettGroupPerformance$df}, N = \Sexpr{groupPerformanceSampleSize}) = \Sexpr{bartlettGroupPerformance$chisq}, p = \Sexpr{round(bartlettGroupPerformance$p.value, digits = 2)). One factor, labelled ``Joint Action Success'' was imposed on the data, which explained \Sexpr{prVar.group PerformanceFA}\% of the overall variance (SS Loading = \Sexpr{SSLoadings.groupPerformanceFA}). $Guttmans \lambda = \Sexpr{round(reliabilityGroupPerformance$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityGroupPerformance$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<groupPerformanceCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcGroupPerformance)
xtable(corstarsl(tlcGroupPerformance)) #label = ``tab:jointActionSuccessCorrTable''
@


\myparagraph{Perceptions of Individual Component Performance}

<<dataReductionIndPerformance, eval=TRUE, echo=FALSE>>=
#indPerformanceVariablesSubset: exclude effectivenessInContact (not relvant)
tlcIndPerformance <- subset(tlc, select = c(indDefense, passingTechnique, supportPlay,
                                                  decisionMaking))
tlcIndPerformance.pca <- princomp(na.omit(tlcIndPerformance))
summary(tlcIndPerformance.pca)
plot(tlcIndPerformance.pca)

tlcIndPerformance.fa <- factanal(~ indDefense + passingTechnique + supportPlay +
                                        decisionMaking +, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.indPerformanceFA <- round(sum(tlcIndPerformance.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.indPerformanceFA <-  round((colSums(tlcIndPerformance.fa$loadings*tlcIndPerformance.fa$loadings)/dim(tlcIndPerformance.fa$loadings)[1]*100), digits = 1)


head(tlcIndPerformance.fa$scores)
tlc <- cbind(tlc, tlcIndPerformance.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "indComponentsPerformance"
indPerformanceMatrix <- cor(tlcIndPerformance, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOindPerformanceMatrix <- KMO(indPerformanceMatrix)
# KMOIndPerformanceMatrix$MSA is the colum in which the overal KMO is stored
bartlettIndPerformance <- cortest.bartlett(indPerformanceMatrix, n = 116) # what is the sample size here?
# ls(BartlettIndPerformance)
# BartlettIndPerformance$chisq, BartlettIndPerformance$df, BartlettIndPerformance$p.value
# sample size? length(unique(tlc$englishName)*2)
indPerformanceSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityIndPerformance <- psych::alpha(indPerformanceMatrix)
cronAlphaIndPerformance <- reliabilityIndPerformance$total[,2]
guttLambdaIndPerformance <- reliabilityIndPerformance$total[,3]
@


Items concerning components of individual components of performance in the invasion drill (1-on-1 defence, passing technique, support play in attack, decision making in attack, and effectiveness in contact) were subjected to EFA.  The variable ``effectiveness in contact'' was removed from analysis as the invasion drill was predominantly a non-contact training drill, and so this item was not relevant to athletes' performance.  Correlations between individual component performance items was very high (all r's > \Sexpr{round(min(indPerformanceMatrix), digits = 2)}), which suggested that one factor was appropriate (see Table ~\ref{tab:indComponentPerformanceCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOindPerformanceMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettIndPerformance$df}, N = \Sexpr{indPerformanceSampleSize}) = \Sexpr{bartlettIndPerformance$chisq}, p = \Sexpr{round(bartlettIndPerformance$p.value, digits = 2)). One factor, labelled ``Individual Performance Components'' was imposed on the data, which explained \Sexpr{prVar.indPerformanceFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.indPerformanceFA}). $Guttmans \lambda = \Sexpr{round(reliabilityIndPerformance$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityIndPerformance$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<indComponentsPerformanceCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcIndPerformance)
xtable(corstarsl(tlcIndPerformance))
# label = tab:indComponentPerformanceCorrTable
@



%Appendix
% This is a follow-up EFA with individual and group performance in one matrix.
%<<dataReductionPerformanceAll, eval=TRUE, echo=FALSE>>=
%min(competenceMatrix)

%# 2. jointActionSuccess and individualComponentPerformance:
%#PerformanceComponentsTogether:
%tlcPerformance <- subset(tlc, select = c(groupDefensiveLine, groupAttackingLine,        groupSupportPlay, groupOnfieldCommunication, indDefense, passingTechnique, supportPlay, decisionMaking, effectivenessInContact))

%tlcPerformance.fa <- factanal(~ groupDefensiveLine + groupAttackingLine + groupSupportPlay + groupOnfieldCommunication +
                              %indDefense + passingTechnique + supportPlay + %decisionMaking + effectivenessInContact, 2,
                                 %data = tlc, rotation = "promax", na.action = na.exclude, %scores = "regression")
%tlcPerformance.fa

%tlcPerformanceMatrix <- cor(tlcPerformance, use = "complete")
%KMO(tlcPerformanceMatrix)
%library(psych)
%cortest.bartlett(tlcPerformanceMatrix, n = 116) # what is the sample size here?
%psych::alpha(tlcPerformanceMatrix)
%@






\myparagraph{Feelings of team click within training group}

<<dataReductionGroupClick, eval=TRUE, echo=FALSE>>=
#GroupClickVariablesSubset: exclude effectivenessInContact (not relvant)
tlcGroupClick <- subset(tlc, select = c(groupReliabilityOfOthers,
                                          groupReliabilityForOthers, groupAbilitiesExtended, groupClickPictorial, groupUnspokenUnderstanding, groupGeneralAtmosphere))
tlcGroupClick.pca <- princomp(na.omit(tlcGroupClick))
summary(tlcGroupClick.pca)
plot(tlcGroupClick.pca)

tlcGroupClick.fa <- factanal(~ groupReliabilityOfOthers + groupAbilitiesExtended +
                                        groupClickPictorial + groupUnspokenUnderstanding + groupGeneralAtmosphere, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.groupClickFA <- round(sum(tlcGroupClick.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.groupClickFA <-  round((colSums(tlcGroupClick.fa$loadings*tlcGroupClick.fa$loadings)/dim(tlcGroupClick.fa$loadings)[1]*100), digits = 1)


head(tlcGroupClick.fa$scores)
tlc <- cbind(tlc, tlcGroupClick.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "groupClickFactor"
groupClickMatrix <- cor(tlcGroupClick, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOgroupClickMatrix <- KMO(groupClickMatrix)
# KMOGroupClickMatrix$MSA is the colum in which the overal KMO is stored
bartlettGroupClick <- cortest.bartlett(groupClickMatrix, n = 116) # what is the sample size here?
# ls(BartlettGroupClick)
# BartlettGroupClick$chisq, BartlettGroupClick$df, BartlettGroupClick$p.value
# sample size? length(unique(tlc$englishName)*2)
groupClickSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityGroupClick <- psych::alpha(groupClickMatrix)
cronAlphaGroupClick <- reliabilityGroupClick$total[,2]
guttLambdaGroupClick <- reliabilityGroupClick$total[,3]
@

Items relating to group team click (emotional support, shared goal, unspoken understanding, general atmosphere, click pictorial, reliability of others, reliability for others, ability extended by the group) were subjected to EFA.  The variable ``reliability for others'' was removed from the matrix because it did not significantly correlate with other variables (all r's <= .16).  Correlations between the remaining were very high (all $r's > \Sexpr{round(min(groupClickMatrix), digits = 2)}>$), which suggested that one factor would be appropriate (see Table ee Table ~\ref{tab:groupClickCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOgroupClickMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettGroupClick$df}, N = \Sexpr{groupClickSampleSize}) = \Sexpr{bartlettGroupClick$chisq}, p = \Sexpr{round(bartlettGroupClick$p.value, digits = 2)). One factor, labelled ``Group Click'' was imposed on the data, which explained \Sexpr{prVar.groupClickFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.groupClickFA}). $Guttmans \lambda = \Sexpr{round(reliabilityGroupClick$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityGroupClick$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<groupClickCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcGroupClick)
xtable(corstarsl(tlcGroupClick))
# label = tab:groupClickCorrTable
@




\myparagraph{Feelings of Social Bonding to the training group}


<<dataReductionGroupBonding, eval=TRUE, echo=FALSE>>=
#GroupBondingVariablesSubset: exclude effectivenessInContact (not relvant)
tlcGroupBonding <- subset(tlc, select = c(groupEmotionalSupport, groupSharedGoal,
                                                groupFusionPictorial))
tlcGroupBonding.pca <- princomp(na.omit(tlcGroupBonding))
summary(tlcGroupBonding.pca)
plot(tlcGroupBonding.pca)

tlcGroupBonding.fa <- factanal(~ groupEmotionalSupport + groupSharedGoal +
                                      groupFusionPictorial, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.groupBondingFA <- round(sum(tlcGroupBonding.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.groupBondingFA <-  round((colSums(tlcGroupBonding.fa$loadings*tlcGroupBonding.fa$loadings)/dim(tlcGroupBonding.fa$loadings)[1]*100), digits = 1)

head(tlcGroupBonding.fa$scores)
tlc <- cbind(tlc, tlcGroupBonding.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "groupBondingFactor"
groupBondingMatrix <- cor(tlcGroupBonding, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOgroupBondingMatrix <- KMO(groupBondingMatrix)
# KMOGroupBondingMatrix$MSA is the colum in which the overal KMO is stored
bartlettGroupBonding <- cortest.bartlett(groupBondingMatrix, n = 116) # what is the sample size here?
# ls(BartlettGroupBonding)
# BartlettGroupBonding$chisq, BartlettGroupBonding$df, BartlettGroupBonding$p.value
# sample size? length(unique(tlc$englishName)*2)
groupBondingSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityGroupBonding <- psych::alpha(groupBondingMatrix)
cronAlphaGroupBonding <- reliabilityGroupBonding$total[,2]
guttLambdaGroupBonding <- reliabilityGroupBonding$total[,3]
@


Items concerning bonding to the training group (emotional support, shared goal, fusion pictorial) were subjected to EFA.  Correlations between items were high (all $r's > \Sexpr{round(min(groupBondingMatrix), digits = 2)}>$), which suggested that one factor would be appropriate (see Table ee Table ~\ref{tab:groupBondingCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOgroupBondingMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettGroupBonding$df}, N = \Sexpr{groupBondingSampleSize}) = \Sexpr{bartlettGroupBonding$chisq}, p = \Sexpr{round(bartlettGroupBonding$p.value, digits = 2)). One factor, labelled ``Group Social Bonding'' was imposed on the data, which explained \Sexpr{prVar.groupBondingFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.groupBondingFA}). $Guttmans \lambda = \Sexpr{round(reliabilityGroupBonding$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityGroupBonding$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<groupBondingCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcGroupBonding)
xtable(corstarsl(tlcGroupBonding))
# label = tab:groupBondingCorrTable
@





%team click
% "teamClickPictorial_1"  # TEAM CLICK post not measured
% [94] "reliabilityForOthers_1" # TEAM RELIABILITY - i don't ask about the team as a whole post-experiment...
% [95] "reliabilityOfOthers_1"
\subsubsection{Social Bonding to the team}


<<dataReductionTeamBonding, eval=TRUE, echo=FALSE>>=
#TeamBondingVariablesSubset: exclude effectivenessInContact (not relvant)
tlcTeamBonding <- subset(tlc, select = c(emotionalSupport, sharedGoal,
                                              fusionPictorialTeam, fusionVerbal, groupId))
tlcTeamBonding.pca <- princomp(na.omit(tlcTeamBonding))
summary(tlcTeamBonding.pca)
plot(tlcTeamBonding.pca)

tlcTeamBonding.fa <- factanal(~ emotionalSupport + sharedGoal +
                                      fusionPictorialTeam + fusionVerbal + groupId, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.teamBondingFA <- round(sum(tlcTeamBonding.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.teamBondingFA <-  round((colSums(tlcTeamBonding.fa$loadings*tlcTeamBonding.fa$loadings)/dim(tlcTeamBonding.fa$loadings)[1]*100), digits = 1)

head(tlcTeamBonding.fa$scores)
tlc <- cbind(tlc, tlcTeamBonding.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "teamBondingFactor"
teamBondingMatrix <- cor(tlcTeamBonding, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOteamBondingMatrix <- KMO(teamBondingMatrix)
# KMOTeamBondingMatrix$MSA is the colum in which the overal KMO is stored
bartlettTeamBonding <- cortest.bartlett(teamBondingMatrix, n = 116) # what is the sample size here?
# ls(BartlettTeamBonding)
# BartlettTeamBonding$chisq, BartlettTeamBonding$df, BartlettTeamBonding$p.value
# sample size? length(unique(tlc$englishName)*2)
teamBondingSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityTeamBonding <- psych::alpha(teamBondingMatrix)
cronAlphaTeamBonding <- reliabilityTeamBonding$total[,2]
guttLambdaTeamBonding <- reliabilityTeamBonding$total[,3]
@


Items concerning bonding to the team (emotional support, shared goal, fusion pictorial, fusion verbal, and group identification) were subjected to EFA.  Correlations between items were all reasonably high (all $r's > \Sexpr{round(min(teamBondingMatrix), digits = 2)}>$), which suggested that one factor would be appropriate (see Table ee Table ~\ref{tab:teamBondingCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOteamBondingMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettTeamBonding$df}, N = \Sexpr{teamBondingSampleSize}) = \Sexpr{bartlettTeamBonding$chisq}, p = \Sexpr{round(bartlettTeamBonding$p.value, digits = 2)). One factor, labelled ``Team Social Bonding'' was imposed on the data, which explained \Sexpr{prVar.teamBondingFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.teamBondingFA}). $Guttmans \lambda = \Sexpr{round(reliabilityTeamBonding$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityTeamBonding$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<teamBondingCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcTeamBonding)
xtable(corstarsl(tlcTeamBonding))
# label = tab:teamBondingCorrTable
@


\myparagraph{Arousal}

<<dataReductionArousal, eval=TRUE, echo=FALSE>>=
#ArousalVariablesSubset: exclude effectivenessInContact (not relvant)
tlcArousal <- subset(tlc, select = c(aroused, relaxed, excited))
tlcArousal.pca <- princomp(na.omit(tlcArousal))
summary(tlcArousal.pca)
plot(tlcArousal.pca)

tlcArousal.fa <- factanal(~ aroused + relaxed + excited, 1, data = tlc, rotation =        "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.ArousalFA <- round(sum(tlcArousal.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.ArousalFA <-  round((colSums(tlcArousal.fa$loadings*tlcArousal.fa$loadings)/dim(tlcArousal.fa$loadings)[1]*100), digits = 1)

head(tlcArousal.fa$scores)
tlc <- cbind(tlc, tlcArousal.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "arousalFactor"
arousalMatrix <- cor(tlcArousal, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOarousalMatrix <- KMO(arousalMatrix)
# KMOarousalMatrix$MSA is the colum in which the overal KMO is stored
bartlettArousal <- cortest.bartlett(arousalMatrix, n = 116) # what is the sample size here?
# ls(BartlettArousal)
# BartlettArousal$chisq, BartlettArousal$df, BartlettArousal$p.value
# sample size? length(unique(tlc$englishName)*2)
arousalSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityArousal <- psych::alpha(arousalMatrix)
cronAlphaArousal <- reliabilityArousal$total[,2]
guttLambdaArousal <- reliabilityArousal$total[,3]
@


Items associated with team discipline (attendance at meals, team curfew, general team conduct, punctuality to team engagements), measured at baseline and after the experiment, were subjected to EFA.  Correlations between items were all reasonably high (all $r's > \Sexpr{round(min(arousalMatrix), digits = 2)}>$), which suggested that one factor would be appropriate (see Table ee Table ~\ref{tab:arousalCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOarousalMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettarousal$df}, N = \Sexpr{arousalSampleSize}) = \Sexpr{bartlettArousal$chisq}, p = \Sexpr{round(bartlettArousal$p.value, digits = 2)). One factor, labelled ``Team Discipline'' was imposed on the data, which explained \Sexpr{prVar.ArousalFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.arousalFA}). $Guttmans \lambda = \Sexpr{round(reliabilityArousal$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityArousal$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<arousalCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcArousal)
xtable(corstarsl(tlcArousal))
# label = tab:arousalCorrTable
@




















\subsubsection{Athlete Technical Competence (objective and subjective measures)}

<<dataReductionTechnicalCompetence, eval=TRUE, echo=FALSE>>=
#TechnicalCompetenceVariablesSubset:
tlcTechnicalCompetence <- subset(tlc, select = c(yearsTeam, trainingAge, teamStatus,
                                          athleteStatus, indAbilityTeammates_1, indAbilityChina_1, indAbilityInternational_1, teamAbilityChina_1))
tlcTechnicalCompetence.pca <- princomp(na.omit(tlcTechnicalCompetence))
summary(tlcTechnicalCompetence.pca)
plot(tlcTechnicalCompetence.pca)

tlcTechnicalCompetence.fa <- factanal(~ yearsTeam + trainingAge  + teamStatus +
                                          athleteStatus + indAbilityTeammates_1 + indAbilityChina_1 + indAbilityInternational_1, 2, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.objectiveCompetenceFA <- round(sum(tlcTechnicalCompetence.fa$loadings[,1]^2),
                                              digits = 2)

SSLoadings.subjectiveCompetenceFA <- round(sum(tlcTechnicalCompetence.fa$loadings[,2]^2),
                                              digits = 2)
#------------------Proportion of Variance----------------------#
prVar.technicalCompetenceFA <-  round((colSums(tlcTechnicalCompetence.fa$loadings*tlcTechnicalCompetence.fa$loadings)/dim(tlcTechnicalCompetence.fa$loadings)[1]*100), digits = 1)

unclass(prVar.technicalCompetenceFA)
prVar.technicalCompetenceFA <- data.frame(prVar.technicalCompetenceFA)
prVar.objectiveCompetenceFA <- prVar.technicalCompetenceFA[1,1]
prVar.subjectiveCompetenceFA <- prVar.technicalCompetenceFA[2,1]


head(tlcTechnicalCompetence.fa$scores)
tlc <- cbind(tlc, tlcTechnicalCompetence.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "objectiveCompetenceFactor"
names(tlc)[names(tlc) == "Factor2"] <- "subjectiveCompetenceFactor"
technicalCompetenceMatrix <- cor(tlcTechnicalCompetence, use = "complete")
objectiveCompetenceMatrix <- cor(subset(tlcTechnicalCompetence, select = c(yearsTeam, trainingAge, teamStatus, athleteStatus)))
subjectiveCompetenceMatrix <- cor(subset(tlcTechnicalCompetence, select = c(indAbilityTeammates_1, indAbilityChina_1, indAbilityInternational_1)))

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOtechnicalCompetenceMatrix <- KMO(technicalCompetenceMatrix)
# KMOTechnicalCompetenceMatrix$MSA is the colum in which the overal KMO is stored
bartlettTechnicalCompetence <- cortest.bartlett(technicalCompetenceMatrix, n = 116) # what is the sample size here?
# ls(BartlettTechnicalCompetence)
# BartlettTechnicalCompetence$chisq, BartlettTechnicalCompetence$df, BartlettTechnicalCompetence$p.value
# sample size? length(unique(tlc$englishName)*2)
technicalCompetenceSampleSize <- length(unique(tlc$englishName)) # values for each participant measured at one time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityTechnicalCompetence <- psych::alpha(technicalCompetenceMatrix)
cronAlphaTechnicalCompetence <- reliabilityTechnicalCompetence$total[,2]
guttLambdaTechnicalCompetence <- reliabilityTechnicalCompetence$total[,3]
@


All eight items relevant to technical competence were analysed in a correlation matrix to assess relatedness (see Table ~\ref{tab:technicalCompetenceCorrTable}). All measures of objective competence were highly correlated all r's >= \Sexpr{round(min(objectiveCompetenceMatrix), digits = 2)}),  and among measures of subjective competence (all items except for the team competence measure correlated at $> \Sexpr{round(min(subjectiveCompetenceMatrix), digits = 2)})$) suggested that the data could be explained by two underlying factors. Team Ability Chinese Provinces was dropped from analysis due to low correlation with other competence variables, possibly because the item did not ask about an individual athlete’s competence (it referred instead to an athlete’s opinion of the competence of the team of which they were a member).

An EFA of technical competence variables revealed that items of interest loaded on two factors. Measures of objective competence (Years Team, Training Age, Team Status, Athlete Status) loaded on the first factor, which was labelled ``Objective Competence'' because the measures were all objective markers of an athlete's competence.
Objective Competence explained \Sexpr{prVar.objectiveCompetenceFA}\% of the total variance (\Sexpr{SSLoadings.objectiveCompetenceFA}). The remaining measures of subjective competence (Ability Teammates, Ability Chinese Pros, Ability International Pros) loaded on the remaining factor.  The second factor was labelled ``Subjective Competence'', due to the fact that all measures were the product of athlete self-report.  Subjective competence explained \Sexpr{prVar.objectiveCompetenceFA}\% of the variance (\Sexpr{SSLoadings.subjectiveCompetenceFA}).
$Guttmans \lambda = \Sexpr{round(reliabilityTechnicalCompetence$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityTechnicalCompetence$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.


<<technicalCompetenceCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcTechnicalCompetence)
xtable(corstarsl(tlcTechnicalCompetence))
# label = tab:technicalCompetenceCorrTable
@

\subsubsection{Team Discipline}



<<dataReductionTeamDiscipline, eval=TRUE, echo=FALSE>>=
#TeamDisciplineVariablesSubset: exclude effectivenessInContact (not relvant)
tlcTeamDiscipline <- subset(tlc, select = c(teamAttendanceMeals, teamCurfew, teamGeneralConduct, teamPunctuality))
tlcTeamDiscipline.pca <- princomp(na.omit(tlcTeamDiscipline))
summary(tlcTeamDiscipline.pca)
plot(tlcTeamDiscipline.pca)

tlcTeamDiscipline.fa <- factanal(~ teamAttendanceMeals + teamCurfew + teamGeneralConduct +
                                      teamPunctuality, 1, data = tlc, rotation = "promax", na.action = na.exclude, scores = "regression")  #<- this formula format allows preserving of NAs for later rebind to df

#-----------------------------SS Loadings----------------------#
SSLoadings.teamDisciplineFA <- round(sum(tlcTeamDiscipline.fa$loadings[,1]^2), digits = 2)
#------------------Proportion of Variance----------------------#
prVar.teamDisciplineFA <-  round((colSums(tlcTeamDiscipline.fa$loadings*tlcTeamDiscipline.fa$loadings)/dim(tlcTeamDiscipline.fa$loadings)[1]*100), digits = 1)

head(tlcTeamDiscipline.fa$scores)
tlc <- cbind(tlc, tlcTeamDiscipline.fa$scores)
names(tlc)[names(tlc) == "Factor1"] <- "teamDisciplineFactor"
teamDisciplineMatrix <- cor(tlcTeamDiscipline, use = "complete")

#---------suitability measures (KMO & Bartlett)----------------#
library(psych)
KMOteamDisciplineMatrix <- KMO(teamDisciplineMatrix)
# KMOTeamDisciplineMatrix$MSA is the colum in which the overal KMO is stored
bartlettTeamDiscipline <- cortest.bartlett(teamDisciplineMatrix, n = 116) # what is the sample size here?
# ls(BartlettTeamDiscipline)
# BartlettTeamDiscipline$chisq, BartlettTeamDiscipline$df, BartlettTeamDiscipline$p.value
# sample size? length(unique(tlc$englishName)*2)
teamDisciplineSampleSize <- length(unique(tlc$englishName))*2 # values for each participant measured attwo time points

#----------reliability measures--------------------------------#
# Cronbach's Alpha and Guttman's Lambda using psych::alpha()
#Cronbach's Alpha is located reliabilityObject$total[,2]
#Guttman's Lambda is located reliabilityObject$total[,3]
reliabilityTeamDiscipline <- psych::alpha(teamDisciplineMatrix)
cronAlphaTeamDiscipline <- reliabilityTeamDiscipline$total[,2]
guttLambdaTeamDiscipline <- reliabilityTeamDiscipline$total[,3]
@


Items concerning team discipline (attendance at meals, team curfew, general team conduct, punctuality to team engagements), measured at baseline and after the experiment, were subjected to EFA.  Correlations between items were all reasonably high (all $r's > \Sexpr{round(min(teamDisciplineMatrix), digits = 2)}>$), which suggested that one factor would be appropriate (see Table ee Table ~\ref{tab:teamDisciplineCorrTable}). The KMO index and Bartlett's test both suggested high sampling adequacy, (KMO =  \Sexpr{round(KMOteamDisciplineMatrix$MSA, digits = 2)}, \chi^2(\Sexpr{bartlettTeamDiscipline$df}, N = \Sexpr{teamDisciplineSampleSize}) = \Sexpr{bartlettTeamDiscipline$chisq}, p = \Sexpr{round(bartlettTeamDiscipline$p.value, digits = 2)). One factor, labelled ``Team Discipline'' was imposed on the data, which explained \Sexpr{prVar.teamDisciplineFA} \% of the overall variance (SS Loading = \Sexpr{SSLoadings.teamDisciplineFA}). $Guttmans \lambda = \Sexpr{round(reliabilityTeamDiscipline$total[,3], digits = 2)}$ and $Cronbachs \alpha = \Sexpr{round(reliabilityTeamDiscipline$total[,2], digits = 2)}$ indicated that the data reduction was appropriate and reliable.

<<teamDisciplineCor, echo = TRUE>>=
# be sure to import the variable subset data frame, not the correlation matrix
corstarsl(tlcTeamDiscipline)
xtable(corstarsl(tlcDiscipline))
# label = tab:teamDisciplineCorrTable
@




% \myparagraph{Fatigue} - fatigue is measured pre and post experiment, but mental and prpe are measured only post-experiment, so no need for EFA

<<factorsAttachWide, echo = FALSE, eval = TRUE >>=
#attach factors to wide-format variable:
# jointActionSuccess:
twc$jointActionSuccessPost <- tlc$jointActionSuccess[tlc$time == 3]
twc$jointActionSuccessPre <- tlc$jointActionSuccess[tlc$time == 2]
# indPerformanceComponents:
twc$indPerformanceComponentsPost <- tlc$indPerformanceComponents[tlc$time == 3]
twc$indPerformanceComponentsPre <- tlc$indPerformanceComponents[tlc$time == 2]
# groupClick
twc$groupClickFactorPost <- tlc$groupClickFactor[tlc$time == 3]
twc$groupClickFactorPre <- tlc$groupClickFactor[tlc$time == 2]
# groupBonding
twc$groupBondingFactorPost <- tlc$groupBondingFactor[tlc$time == 3]
twc$groupBondingFactorPre <- tlc$groupBondingFactor[tlc$time == 2]
#arousalFactor
twc$arousalFactorPre <- tlc$arousalFactor[tlc$time == 2]
twc$arousalFactorPost <- tlc$arousalFactor[tlc$time == 3]
#objectiveCompetence:
twc$objectiveCompetenceFactor <- tlc$objectiveCompetenceFactor[tlc$time == 1]
#subjectiveCompetence:
twc$subjectiveCompetenceFactor <- tlc$subjectiveCompetenceFactor[tlc$time == 1]
#teamBondingFactor:
twc$teamBondingFactor <- tlc$teamBondingFactor[tlc$time == 1]
#teamDisciplineFactor:
twc$teamDisciplineFactor <- tlc$teamDisciplineFactor[tlc$time == 1]
@



\subsection{Manipulation Checks}
Athletes were asked two questions before the experiment to gauge their confidence in individual and group ability to handle the technical challenges of the training drill. These measures were used as an indication of the effectiveness of the experimental manipulation.

Neither group () nor individual confidence in ability to meet technical challenges differed significantly by condition.  The average

<<manipulationChecks, echo = FALSE, eval = TRUE>>=
#condition differences in:
# 1. groupConfidentTechnicalChallenges,
# 2. indConfidenceChallenges,
# 3. arousal

#confidence in challenges
boxplot(groupConfidentTechnicalChallenges_2 ~ condition, data = twc)
install.packages("vioplot")
library(vioplot)
vioplot(twc$groupConfidentTechnicalChallenges_2, twc$condition, horizontal=F, col="gray")

library(Hmisc)
describe(groupConfidentTechnicalChallenges_2[condition == ``low''], data = twc, na.action = na.omit)

boxplot(indConfidenceChallenges_2 ~ condition, data = twc)
groupConfidencePre <- summary(lm(twc$groupConfidentTechnicalChallenges_2 ~ twc$condition + twc$indConfidenceChallenges_2))

#arousal
boxplot(arousalFactorPre ~ condition, data = twc)
  arousalPre <- summary(lm(twc$arousalFactorPre ~ twc$condition))
SD(twc$arousalFactorPre[twc$condition == ``low''] )
sd(na.rm(twc$arousalFactorPre[twc$condition == ``high'']))

#groupConfidentTechnicalChallenges
hist(tlc$groupConfidentTechnicalChallenges[tlc$condition == "low"])
hist(tlc$groupConfidentTechnicalChallenges[tlc$condition == "high"])
t.test(tlc$groupConfidentTechnicalChallenges[tlc$condition == "high"], tlc$groupConfidentTechnicalChallenges[tlc$condition == "low"], paired = F)

#indConfidenceChallenges
hist(tlc$indConfidenceChallenges[tlc$condition == "low" & tlc$time == 2])
hist(tlc$indConfidenceChallenges[tlc$condition == "low" & tlc$time == 3])
hist(tlc$indConfidenceChallenges[tlc$condition == "high"])
hist(tlc$indConfidenceChallenges[tlc$condition == "high" & tlc$time == 2])
hist(tlc$indConfidenceChallenges[tlc$condition == "high" & tlc$time == 3])
t.test(tlc$indConfidenceChallenges[tlc$condition == "high"], tlc$groupConfidentTechnicalChallenges[tlc$condition == "low"], paired = T)
@





























<<prePostChanges>>=
library(lme4)
library(optimx)

#bondingTime
summary(bondingTimeInt.lmer <- lmer(groupBondingFactor ~ time + ( 1 | englishName/team),
                                    data =  trainingLong, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE           ,                                      optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))


#condTimeInteraction
bondingCondTimeInt.lmer <- lmer(groupBondingFactor ~ time*condition + ( 1 | englishName/team),
                                data =  trainingLong, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                          optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE)))
summary(bondingCondTime.lmer)




summary(bondingPostCond.lmer <- lmer(groupBondingPost ~ condition + ( condition | team),
                                     data =  training, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                           optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))
@




<<predictionsTest>>=
# prediction 1a: joint-action success -> teamClick

plot(twc$groupPerformancePost, twc$groupClickPost)
abline(lm(twc$groupClickPost ~ twc$groupPerformancePost))
summary(lm(twc$groupClickPost ~ twc$groupPerformancePost))

library(lme4)
library(optimx)
summary(clickPostTeamPerformance.lmer <- lmer(groupClickPost ~ groupPerformancePost + condition + indPerformancePost + ( 1 | team),
                                              data =  twc, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                                    optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))


# prediction 1b: groupPerformanceExpectations -> teamClick:
twc$groupPerformance_3Norm <- scale(twc$groupPerformance_3)
twc$indPerformance_3Norm <- scale(twc$indPerformance_3)

plot(twc$groupPerformance_3Norm, twc$groupClickPost)
abline(lm(twc$groupClickPost ~ twc$groupPerformance_3Norm))
summary(lm(twc$groupClickPost ~ twc$groupPerformance_3Norm))
summary(clickPostTeamPerformanceExpectations.lmer <- lmer(groupClickPost ~ groupPerformance_3Norm + condition + indPerformance_3Norm +  ( groupPerformance_3Norm | team),
                                                          data =  twc, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                                                optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))

#prediction 1c: jointActionSuccess*groupPerformanceExpectations:


summary(clickPostTeamPerformanceExpInt.lmer <- lmer(groupClickPost ~ groupPerformancePost*groupPerformance_3Norm + condition + ( groupPerformancePost | team),
                                                    data =  twc, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                                          optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))


#prediction 2a: groupClick -> groupBonding
plot(twc$groupClickPost, twc$groupBondingPost)
abline(lm(twc$groupBondingPost ~ twc$groupClickPost))
summary(lm(twc$groupBondingPost ~ twc$groupClickPost))

summary(bondingPostClick.lmer <- lmer(groupBondingPost ~ groupClickPost + condition + (groupClickPost | team),
                                      data =  twc, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                            optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))

#prediction 3b:
plot(twc$groupClickPost, twc$groupBondingPost)
abline(lm(twc$groupBondingPost ~ twc$groupClickPost))
summary(lm(twc$groupBondingPost ~ twc$groupClickPost))


summary(bondingPostTeamPerformanceExpectations.lmer <- lmer(groupBondingPost ~ groupPerformance_3Norm + condition + ( groupPerformance_3 | team),
                                                            data =  twc, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                                                  optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))


#prediction 3a: jointActionSuccess -> socialBonding
@



<<exploreBonding>>=
# bonding by session? by team? (sd/bj?)

plot(training$sessionName, training$groupBondingPost)
plot(training$sessionName, training$groupClickPost)
plot(training$team, training$groupBondingPost)
plot(training$team, training$groupClickPost)
plot(training$condition, training$groupBondingPost)
plot(training$condition, training$groupClickPost)
training$sex <- as.factor(training$sex)
plot(training$sex, training$groupBondingPost, type = boxplot)
plot(training$sex, training$groupClickPost)

training$athleteStatus <- as.factor(training$athleteStatus)
plot(training$athleteStatus, training$groupBondingPost)
abline(lm(training$groupBondingPost ~ training$athleteStatus))
plot(training$athleteStatus, training$groupClickPost)

length(unique(training$englishName[training$athleteStatus == 0]))


plot(training$trainingAge, training$groupBondingPost)
abline(lm(training$groupBondingPost ~ training$trainingAge))


training$groupStayChange_3.reversed <- 100 - training$groupStayChange_3

plot(training$groupClickPost, training$groupStayChange_3.reversed)
abline(lm(training$groupStayChange_3.reversed ~ training$groupClickPost))
summary(lm(training$groupStayChange_3.reversed ~ training$groupClickPost))


summary(bondingPostTeamPerformanceExpectations.lmer <- lmer(groupBondingPost ~ groupPerformance_3 + condition + ( groupPerformance_3 | team),
                                                            data =  training, REML = FALSE, control = lmerControl(optimizer = "optimx", calc.derivs = FALSE,
                                                                                                                  optCtrl = list(method = "nlminb", starttests = FALSE, kkt = FALSE))))


#-----------------------EXPLORE RELATIONSHIPS----------------------------------------------------#

plot(twc$groupPerformancePost, twc$groupClickPost)
abline(lm(twc$groupClickPost ~ twc$groupPerformancePost))

#successTotals
plot(twc$successTotals, twc$groupPerformancePost)
abline(lm(twc$groupPerformancePost ~ twc$successTotals))
summary(lm(twc$groupPerformancePost ~ twc$successTotals))


#outcomeAvg:
plot(twc$outcomeAvg, twc$groupPerformancePost)
abline(lm(twc$groupPerformancePost ~ twc$outcomeAvg))
summary(lm(twc$groupPerformancePost ~ twc$outcomeAvg))


#fullOutcomeAvg
plot(twc$fullOutcomeAvg, twc$groupPerformancePost)
abline(lm(twc$groupPerformancePost ~ twc$fullOutcomeAvg))

#fullOutcomeSd  <- this is significant*
plot(twc$fullOutcomeSd, twc$groupPerformancePost)
abline(lm(twc$groupPerformancePost ~ twc$fullOutcomeSd))
summary(lm(twc$groupPerformancePost ~ twc$fullOutcomeSd))

plot(twc$fullOutcomeAvg, twc$groupPerformancePost)
abline(lm(twc$groupPerformancePost ~ twc$fullOutcomeAvg))
summary(lm(twc$groupPerformancePost ~ twc$fullOutcomeAvg))


#prePost changes groupClick:
plot(tlc$time, tlc$groupClickFactor)
boxplot(groupBondingFactor ~ time, tlc)
summary(lm(tlc$groupBondingFactor ~ tlc$time))


#prePost changes grouBonding:
plot(tlc$time, tlc$groupBondingFactor)
boxplot(groupBondingFactor ~ time, tlc)
summary(lm(tlc$groupBondingFactor ~ tlc$time))


boxplot(groupClickPost ~ athleteStatus, twc)
plot(tlc$athleteStatus, tlc$groupClickFactor)
@











\section{Discussion}

1.Summary of the research:

2.Theoretical implications:
Do the results provide support for any existing theories? If not, how can they be explained? Although you do not have to provide a definitive explanation or detailed theory for your results, you at least need to outline one or more possible explanations.

3.Practical implications:
In applied research—and often in basic research—there is also some discussion of the practical implications of the research. How can the results be used, and by whom, to accomplish some real-world goal?

4.Limitations:
The theoretical and practical implications are often followed by a discussion of the study’s limitations. Perhaps there are problems with its internal or external validity. Perhaps the manipulation was not very effective or the measures not very reliable. Perhaps there is some evidence that participants did not fully understand their task or that they were suspicious of the intent of the researchers. Now is the time to discuss these issues and how they might have affected the results.

5.Suggestions for future research:
If the study did not satisfactorily answer the original research question, what will it take to do so? What new research questions has the study raised? This part of the discussion, however, is not just a list of new questions. It is a discussion of two or three of the most important unresolved issues. This means identifying and clarifying each question, suggesting some alternative answers, and even suggesting ways they could be studied.

\section{Appendices}
Appendixes, Tables, and Figures
Appendixes, tables, and figures come after the references. An appendix is appropriate for supplemental material that would interrupt the flow of the research report if it were presented within any of the major sections. An appendix could be used to present lists of stimulus words, questionnaire items, detailed descriptions of special equipment or unusual statistical analyses, or references to the studies that are included in a meta-analysis. Each appendix begins on a new page.



\end{document}
